{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "s2jtmqw22elrnyfjwabm",
   "authorId": "405902089195",
   "authorName": "LALIT",
   "authorEmail": "lalit.kumar@nihilent.com",
   "sessionId": "082ad4b9-05a6-4d52-9ca1-174dd6afb6ce",
   "lastEditTime": 1750081977048
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "bd399dc0-4526-440c-8209-277c20c4cb52",
   "metadata": {
    "language": "python",
    "name": "Session"
   },
   "outputs": [],
   "source": "from snowflake.snowpark import Session\nfrom snowflake.snowpark.types import StructType, StructField, StringType, IntegerType, FloatType, DateType\nfrom snowflake.snowpark.functions import col, count, sum as sum_, count_distinct, concat, lit, when, max as max_\nfrom datetime import datetime, timedelta\nfrom faker import Faker\nimport pandas as pd\nimport random\nimport time\n\n\n# Connect to Snowflake\nconnection_parameters = {\n    \"account\" : \"FVQCWWK-IJB71419\",\n    \"user\" : \"LALIT\",\n    \"authenticator\" : \"Klalitkumar@2025\",\n    \"role\" : \"ACCOUNTADMIN\",\n    \"warehouse\" : \"SNOWFLAKE_LEARNING_WH\",\n    \"database\" : \"SNOWFLAKE_LEARNING_DB\",\n    \"schema\" : \"ODS\"\n}\n\nsession = Session.builder.configs(connection_parameters).create()\n\nSTART_DATE = datetime(2024, 6, 7)\nEND_DATE = datetime(2025, 6, 7)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e544b852-ab42-4539-9cc1-4cf612da1a48",
   "metadata": {
    "language": "python",
    "name": "create_schema",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "def create_schemas(session):\n    import time\n    start_time = time.time()\n\n    try:\n        def schema_exists_with_table(schema_name, table_name):\n            try:\n                session.sql(f\"SELECT 1 FROM SNOWFLAKE_LEARNING_DB.{schema_name}.{table_name} LIMIT 1\").collect()\n                return True\n            except:\n                return False\n\n        # Check STG using DEALER table\n        if not schema_exists_with_table(\"STG\", \"DEALER_PRODUCT_COMBINATIONS\"):\n            session.sql(\"CREATE SCHEMA IF NOT EXISTS SNOWFLAKE_LEARNING_DB.STG\").collect()\n            print(\"Created schema STG\")\n        else:\n            print(\"Schema STG already exists (checked via STG.DEALER_PRODUCT_COMBINATIONS)\")\n\n        # Check DWH using RECOMMENDATION_INPUT table\n        if not schema_exists_with_table(\"DWH\", \"RECOMMENDATION_INPUT\"):\n            session.sql(\"CREATE SCHEMA IF NOT EXISTS SNOWFLAKE_LEARNING_DB.DWH\").collect()\n            print(\"Created schema DWH\")\n        else:\n            print(\"Schema DWH already exists (checked via DWH.RECOMMENDATION_INPUT)\")\n\n        print(f\"create_schemas completed in {time.time() - start_time:.2f} seconds\")\n\n    except Exception as e:\n        print(f\"Error creating schemas: {str(e)}\")\n        raise\n\ncreate_schemas(session)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5ab347a6-0f5b-4ea4-87b8-4623f30bd9fb",
   "metadata": {
    "language": "python",
    "name": "get_data",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "def read_data(session):\n    start_time = time.time()\n    try:\n        sales_df = session.table(\"SNOWFLAKE_LEARNING_DB.ODS.SALES\").filter(\n            (col(\"DATE\") >= START_DATE) & (col(\"DATE\") <= END_DATE) &\n            (col(\"QUANTITY\") > 0)\n        )\n        dealers_df = session.table(\"SNOWFLAKE_LEARNING_DB.ODS.DEALERS\")\n        products_df = session.table(\"SNOWFLAKE_LEARNING_DB.ODS.PRODUCTS\")\n        dp_mapping_df = session.table(\"SNOWFLAKE_LEARNING_DB.ODS.DP_MAPPING\")\n        ec_club_df = session.table(\"SNOWFLAKE_LEARNING_DB.ODS.EC_CLUB\")\n\n        # Log schemas and sample data for debugging\n        print(f\"SALES schema (Snowpark): {sales_df.columns}\")\n        print(f\"DEALERS schema (Snowpark): {dealers_df.columns}\")\n        print(f\"PRODUCTS schema (Snowpark): {products_df.columns}\")\n        print(f\"DP_MAPPING schema (Snowpark): {dp_mapping_df.columns}\")\n        print(f\"EC_CLUB schema (Snowpark): {ec_club_df.columns}\")\n        sales_schema = session.sql(\"DESCRIBE TABLE SNOWFLAKE_LEARNING_DB.ODS.SALES\").to_pandas()\n        dealers_schema = session.sql(\"DESCRIBE TABLE SNOWFLAKE_LEARNING_DB.ODS.DEALERS\").to_pandas()\n        print(f\"SALES table columns (Snowflake): {sales_schema.columns.tolist()}\")\n        print(f\"DEALERS table columns (Snowflake): {dealers_schema.columns.tolist()}\")\n        # Log sample data\n        sales_sample = sales_df.limit(5).to_pandas()\n        dealers_sample = dealers_df.limit(5).to_pandas()\n        print(f\"SALES sample data:\\n{sales_sample}\")\n        print(f\"DEALERS sample data:\\n{dealers_sample}\")\n\n        print(f\"read_data completed in {time.time() - start_time:.2f} seconds\")\n        return sales_df, dealers_df, products_df, dp_mapping_df, ec_club_df\n    except Exception as e:\n        print(f\"Error reading data: {str(e)}\")\n        raise\n\nsales_df, dealers_df, products_df, dp_mapping_df, ec_club_df = read_data(session)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "047e5a93-39a6-4fdc-b4a1-0d01deecabd3",
   "metadata": {
    "language": "python",
    "name": "cleaning_and_joining",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "def clean_and_join_data(sales_df, dealers_df, products_df, dp_mapping_df, ec_club_df):\n    start_time = time.time()\n    try:\n        # Log input table row counts and samples\n        print(f\"SALES row count: {sales_df.count()}\")\n        print(f\"DEALERS row count: {dealers_df.count()}\")\n        print(f\"PRODUCTS row count: {products_df.count()}\")\n        print(f\"SALES sample:\\n{sales_df.limit(5).to_pandas()}\")\n        print(f\"DEALERS sample:\\n{dealers_df.limit(5).to_pandas()}\")\n        print(f\"PRODUCTS sample:\\n{products_df.limit(5).to_pandas()}\")\n\n        # Join sales with dealers\n        processed_df = sales_df.join(\n            dealers_df,\n            sales_df[\"DEALER_NO\"] == dealers_df[\"DEALER_NO\"],\n            \"left\"\n        ).select(\n            sales_df[\"DEALER_NO\"].alias(\"DEALER_NO\"),\n            sales_df[\"SKU\"].alias(\"SKU\"),\n            sales_df[\"DATE\"].alias(\"DATE\"),\n            sales_df[\"QUANTITY\"].alias(\"QUANTITY\"),\n            sales_df[\"AMOUNT\"].alias(\"AMOUNT\"),\n            dealers_df[\"DEALER_NAME\"].alias(\"DEALER_NAME\"),\n            dealers_df[\"REGION\"].alias(\"REGION\"),\n            dealers_df[\"ADDRESS\"].alias(\"ADDRESS\"),\n            dealers_df[\"ANNUAL_REVENUE\"].alias(\"ANNUAL_REVENUE\"),\n            dealers_df[\"DEALER_TYPE\"].alias(\"DEALER_TYPE\"),\n            dealers_df[\"EC_CLUB\"].alias(\"EC_CLUB\")\n        )\n\n        print(f\"processed_df schema after sales-dealers join: {processed_df.columns}\")\n        print(f\"processed_df row count after sales-dealers join: {processed_df.count()}\")\n\n        # Join with dp_mapping\n        processed_df = processed_df.join(\n            dp_mapping_df,\n            processed_df[\"DEALER_NO\"] == dp_mapping_df[\"DN_NUMBER\"],\n            \"left\"\n        ).select(\n            processed_df[\"*\"],\n            dp_mapping_df[\"VERTICAL\"].alias(\"VERTICAL\")\n        )\n        print(f\"processed_df schema after dp_mapping join: {processed_df.columns}\")\n        print(f\"processed_df row count after dp_mapping join: {processed_df.count()}\")\n\n        # Check SKU overlap\n        sales_skus = processed_df.select(col(\"SKU\")).distinct()\n        products_skus = products_df.select(col(\"SKU\")).distinct()\n        common_skus = sales_skus.join(products_skus, sales_skus[\"SKU\"] == products_skus[\"SKU\"], \"inner\")\n        print(f\"Common SKU count: {common_skus.count()}\")\n        unmatched_sales_skus = sales_skus.join(products_skus, sales_skus[\"SKU\"] == products_skus[\"SKU\"], \"left_anti\")\n        print(f\"Unmatched SKU in processed_df sample:\\n{unmatched_sales_skus.limit(5).to_pandas()}\")\n\n        # Join with products and filter valid trading/finished goods\n        processed_df = processed_df.join(\n            products_df,\n            processed_df[\"SKU\"] == products_df[\"SKU\"],\n            \"left\"\n        ).filter(col(\"IS_TRADING\") == 1\n        ).select(\n            processed_df[\"DEALER_NO\"].alias(\"DEALER_NO\"),\n            processed_df[\"SKU\"].alias(\"SKU\"),\n            processed_df[\"DATE\"].alias(\"DATE\"),\n            processed_df[\"QUANTITY\"].alias(\"QUANTITY\"),\n            processed_df[\"AMOUNT\"].alias(\"AMOUNT\"),\n            processed_df[\"DEALER_NAME\"].alias(\"DEALER_NAME\"),\n            processed_df[\"REGION\"].alias(\"REGION\"),\n            processed_df[\"ADDRESS\"].alias(\"ADDRESS\"),\n            processed_df[\"ANNUAL_REVENUE\"].alias(\"ANNUAL_REVENUE\"),\n            processed_df[\"DEALER_TYPE\"].alias(\"DEALER_TYPE\"),\n            processed_df[\"EC_CLUB\"].alias(\"EC_CLUB\"),\n            processed_df[\"VERTICAL\"].alias(\"VERTICAL\"),\n            products_df[\"PRODUCT_NAME\"].alias(\"PRODUCT_NAME\"),\n            products_df[\"PROD_CATEGORY\"].alias(\"PROD_CATEGORY\"),\n            products_df[\"PROD_RANGE\"].alias(\"PROD_RANGE\"),\n            products_df[\"PROD_SUBCATEGORY\"].alias(\"PROD_SUBCATEGORY\")\n        ).drop(\"IS_TRADING\", \"IS_FINISHED\")\n            \n        print(f\"processed_df schema after products join: {processed_df.columns}\")\n        print(f\"processed_df row count after products join: {processed_df.count()}\")\n\n        # Cache to optimize subsequent operations\n        try:\n            processed_df = processed_df.cache_result()\n            print(\"Successfully cached processed_df\")\n        except Exception as cache_e:\n            print(f\"cache_result failed: {str(cache_e)}. Persisting to temporary table.\")\n            processed_df.write.mode(\"overwrite\").save_as_table(\"SNOWFLAKE_LEARNING_DB.STG.TEMP_PROCESSED_DF\")\n            processed_df = session.table(\"SNOWFLAKE_LEARNING_DB.STG.TEMP_PROCESSED_DF\")\n            print(\"Loaded processed_df from temporary table\")\n\n        print(f\"clean_and_join_data completed in {time.time() - start_time:.2f} seconds\")\n        return processed_df\n\n    except Exception as e:\n        print(f\"Error cleaning and joining data: {str(e)}\")\n        raise\n\nprocessed_df = clean_and_join_data(sales_df, dealers_df, products_df, dp_mapping_df, ec_club_df)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8ab2921e-a8f1-4563-9731-0ec0dbb45783",
   "metadata": {
    "language": "python",
    "name": "features_compute",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "\ndef compute_features(processed_df):\n    start_time = time.time()\n    try:\n        dealer_features = processed_df.group_by(col(\"DEALER_NO\")).agg(\n            sum_(col(\"AMOUNT\")).alias(\"TOTAL_SALES\"),\n            count(\"*\").alias(\"TOTAL_INVOICES\"),\n            count_distinct(col(\"SKU\")).alias(\"UNIQUE_SKUS\"),\n            sum_(col(\"ANNUAL_REVENUE\")).alias(\"ANNUAL_REVENUE\"),\n            count_distinct(col(\"DEALER_TYPE\")).alias(\"DEALER_TYPE_COUNT\"),\n            count_distinct(col(\"EC_CLUB\")).alias(\"EC_CLUB_COUNT\"),\n            max_(col(\"DEALER_NAME\")).alias(\"DEALER_NAME\"),\n            max_(col(\"REGION\")).alias(\"REGION\"),\n            max_(col(\"ADDRESS\")).alias(\"ADDRESS\")\n        ).with_column(\n            \"AVERAGE_BILL_VALUE\", when(col(\"TOTAL_INVOICES\") > 0, col(\"TOTAL_SALES\") / col(\"TOTAL_INVOICES\")).otherwise(0)\n        ).with_column(\n            \"AVERAGE_SKUS_PER_INVOICE\", when(col(\"TOTAL_INVOICES\") > 0, col(\"UNIQUE_SKUS\") / col(\"TOTAL_INVOICES\")).otherwise(0)\n        )\n\n        product_combinations = processed_df.group_by(\n            col(\"DEALER_NO\"), col(\"PROD_CATEGORY\"), col(\"PROD_RANGE\"), col(\"PROD_SUBCATEGORY\")\n        ).agg(count(\"*\").alias(\"PURCHASE_COUNT\")).select(\n            col(\"DEALER_NO\"),\n            concat(col(\"PROD_CATEGORY\"), lit(\"_\"), col(\"PROD_RANGE\"), lit(\"_\"), col(\"PROD_SUBCATEGORY\")).alias(\"PRODUCT_COMBINATION\")\n        )\n\n        purchase_matrix = processed_df.group_by(\"DEALER_NO\", \"SKU\").agg(\n            count(\"*\").alias(\"PURCHASE_COUNT\")\n        ).select(\n            col(\"DEALER_NO\"), col(\"SKU\"), when(col(\"PURCHASE_COUNT\") > 0, 1).otherwise(0).alias(\"PURCHASED\")\n        )\n\n        sku_plus_one_matrix = processed_df.group_by(\n            col(\"DEALER_NO\"), col(\"PROD_CATEGORY\"), col(\"PROD_RANGE\"), col(\"PROD_SUBCATEGORY\")\n        ).agg(\n            count(\"*\").alias(\"PURCHASE_COUNT\")\n        ).select(\n            col(\"DEALER_NO\"),\n            concat(col(\"PROD_CATEGORY\"), lit(\"_\"), col(\"PROD_RANGE\"), lit(\"_\"), col(\"PROD_SUBCATEGORY\")).alias(\"PRODUCT_COMBINATION\"),\n            when(col(\"PURCHASE_COUNT\") > 0, 1).otherwise(0).alias(\"PURCHASED\")\n        )\n\n        print(f\"compute_features completed in {time.time() - start_time:.2f} seconds\")\n        return dealer_features, product_combinations, purchase_matrix, sku_plus_one_matrix\n    except Exception as e:\n        print(f\"Error computing features: {str(e)}\")\n        raise\n\ndealer_features, product_combinations, purchase_matrix, sku_plus_one_matrix = compute_features(processed_df)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b91cb9a7-6b7c-4030-acae-e85c93ef6ab4",
   "metadata": {
    "language": "python",
    "name": "Save_transformed_data",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "def save_transformed_tables(session, processed_df, dealer_features, product_combinations, purchase_matrix, sku_plus_one_matrix):\n    start_time = time.time()\n    try:\n        processed_df.write.mode(\"overwrite\").save_as_table(\"SNOWFLAKE_LEARNING_DB.STG.PROCESSED_SALES\")\n        dealer_features.write.mode(\"overwrite\").save_as_table(\"SNOWFLAKE_LEARNING_DB.STG.DEALER_FEATURES\")\n        product_combinations.write.mode(\"overwrite\").save_as_table(\"SNOWFLAKE_LEARNING_DB.STG.DEALER_PRODUCT_COMBINATIONS\")\n        purchase_matrix.write.mode(\"overwrite\").save_as_table(\"SNOWFLAKE_LEARNING_DB.STG.PURCHASE_MATRIX\")\n        sku_plus_one_matrix.write.mode(\"overwrite\").save_as_table(\"SNOWFLAKE_LEARNING_DB.STG.SKU_PLUS_ONE_MATRIX\")\n        print(\"Saved transformed tables to STG schema\")\n\n        session.sql(\"\"\"\n            CREATE OR REPLACE TABLE SNOWFLAKE_LEARNING_DB.STG.DEALERS AS\n            SELECT \n                d.DEALER_NO, \n                d.DEALER_NAME, \n                d.REGION, \n                d.ADDRESS, \n                d.ANNUAL_REVENUE, \n                d.DEALER_TYPE, \n                d.EC_CLUB,\n                COALESCE(f.TOTAL_SALES, 0) AS TOTAL_SALES,\n                COALESCE(f.TOTAL_INVOICES, 0) AS TOTAL_INVOICES,\n                COALESCE(f.AVERAGE_BILL_VALUE, 0) AS AVERAGE_BILL_VALUE,\n                COALESCE(f.AVERAGE_SKUS_PER_INVOICE, 0) AS AVERAGE_SKUS_PER_INVOICE\n            FROM SNOWFLAKE_LEARNING_DB.ODS.DEALERS d\n            LEFT JOIN SNOWFLAKE_LEARNING_DB.STG.DEALER_FEATURES f\n            ON d.DEALER_NO = f.DEALER_NO\n        \"\"\").collect()\n        print(\"Updated DEALERS table in STG schema\")\n        print(f\"save_transformed_tables completed in {time.time() - start_time:.2f} seconds\")\n    except Exception as e:\n        print(f\"Error saving transformed tables: {str(e)}\")\n        raise\n\nsave_transformed_tables(session, processed_df, dealer_features, product_combinations, purchase_matrix, sku_plus_one_matrix)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "270639ae-c280-4703-bc14-7c5e5559e588",
   "metadata": {
    "language": "python",
    "name": "Create_DWH_tables",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "\ndef create_dwh_table_and_validate(session, processed_df, dealer_features):\n    start_time = time.time()\n    try:\n        # Force all column names to uppercase\n        processed_df = processed_df.select(*[col(c).alias(c.upper()) for c in processed_df.schema.names])\n        dealer_features = dealer_features.select(*[col(c).alias(c.upper()) for c in dealer_features.schema.names])\n\n        # Join and prepare data\n        dwh_input = processed_df.select(\n            col(\"DEALER_NO\"),\n            col(\"SKU\"),\n            col(\"PRODUCT_NAME\"),\n            concat(col(\"PROD_CATEGORY\"), lit(\"_\"), col(\"PROD_RANGE\"), lit(\"_\"), col(\"PROD_SUBCATEGORY\")).alias(\"PRODUCT_COMBINATION\"),\n            col(\"QUANTITY\"),\n            col(\"AMOUNT\"),\n            col(\"DEALER_NAME\"),\n            col(\"REGION\"),\n            col(\"ADDRESS\"),\n            col(\"ANNUAL_REVENUE\"),\n            col(\"DEALER_TYPE\"),\n            col(\"EC_CLUB\"),\n            col(\"VERTICAL\")\n        ).join(\n            dealer_features.select(\n                col(\"DEALER_NO\"),\n                col(\"TOTAL_SALES\"),\n                col(\"TOTAL_INVOICES\"),\n                col(\"AVERAGE_BILL_VALUE\"),\n                col(\"AVERAGE_SKUS_PER_INVOICE\")\n            ),\n            on=\"DEALER_NO\",\n            how=\"left\"\n        )\n\n        # Save table\n        dwh_input.write.mode(\"overwrite\").save_as_table(\"SNOWFLAKE_LEARNING_DB.DWH.RECOMMENDATION_INPUT\")\n        print(\"Saved RECOMMENDATION_INPUT table to DWH schema\")\n\n        # Duplicate check\n        duplicates = session.sql(\"\"\"\n            SELECT DEALER_NO, SKU, DATE, COUNT(*)\n            FROM SNOWFLAKE_LEARNING_DB.STG.PROCESSED_SALES\n            GROUP BY DEALER_NO, SKU, DATE\n            HAVING COUNT(*) > 1\n        \"\"\").collect()\n        if duplicates:\n            print(f\"Duplicates found in STG.PROCESSED_SALES:\\n{duplicates}\")\n        else:\n            print(\"No duplicates found in STG.PROCESSED_SALES\")\n\n        # Null check\n        null_counts = processed_df.select(\n            count(when(col(\"DEALER_NO\").is_null(), 1)).alias(\"NULL_DEALER_NO\"),\n            count(when(col(\"SKU\").is_null(), 1)).alias(\"NULL_SKU\"),\n            count(when(col(\"AMOUNT\").is_null(), 1)).alias(\"NULL_AMOUNT\")\n        ).collect()[0]\n        print(f\"Null counts in PROCESSED_SALES:\\n{null_counts}\")\n\n        # Sample output\n        sample_dwh = session.table(\"SNOWFLAKE_LEARNING_DB.DWH.RECOMMENDATION_INPUT\").limit(10).to_pandas()\n        print(\"Sample of DWH.RECOMMENDATION_INPUT table:\\n\" + str(sample_dwh))\n\n        print(f\"create_dwh_table_and_validate completed in {time.time() - start_time:.2f} seconds\")\n    except Exception as e:\n        print(f\"Error creating DWH table or validating: {str(e)}\")\n        raise\n\ncreate_dwh_table_and_validate(session, processed_df, dealer_features)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ec2140df-4fba-4a21-8013-ec5dbe17801c",
   "metadata": {
    "language": "python",
    "name": "Collaborative_filtering_"
   },
   "outputs": [],
   "source": "def collaborative_filtering(session):\n    start_time = time.time()\n    try:\n        # Switch to STG schema\n        session.sql(\"USE SCHEMA SNOWFLAKE_LEARNING_DB.STG\").collect()\n        print(\"Switched to STG schema\")\n\n        # Load and filter purchase matrix\n        purchase_matrix = session.table(\"SNOWFLAKE_LEARNING_DB.STG.PURCHASE_MATRIX\")\n        print(f\"Purchase matrix schema: {purchase_matrix.columns}\")\n        row_count = purchase_matrix.count()\n        print(f\"Purchase matrix row count: {row_count}\")\n        if row_count == 0:\n            raise ValueError(\"PURCHASE_MATRIX is empty. Check clean_and_join_data and compute_features.\")\n\n        \n        # Filter active dealers (≥3 purchases) and SKUs (≥3 dealers)\n        session.sql(\"\"\"\n            CREATE OR REPLACE TEMPORARY TABLE SNOWFLAKE_LEARNING_DB.STG.TEMP_ACTIVE_DEALERS AS\n            SELECT DEALER_NO\n            FROM SNOWFLAKE_LEARNING_DB.STG.PURCHASE_MATRIX\n            GROUP BY DEALER_NO\n            HAVING SUM(PURCHASED) >= 3\n        \"\"\").collect()\n        session.sql(\"\"\"\n            CREATE OR REPLACE TEMPORARY TABLE SNOWFLAKE_LEARNING_DB.STG.TEMP_ACTIVE_SKUS AS\n            SELECT SKU\n            FROM SNOWFLAKE_LEARNING_DB.STG.PURCHASE_MATRIX\n            GROUP BY SKU\n            HAVING COUNT(DISTINCT DEALER_NO) >= 3\n        \"\"\").collect()\n        session.sql(\"\"\"\n            CREATE OR REPLACE TEMPORARY TABLE SNOWFLAKE_LEARNING_DB.STG.TEMP_FILTERED_PURCHASE_MATRIX AS\n            SELECT PM.DEALER_NO, PM.SKU, PM.PURCHASED\n            FROM SNOWFLAKE_LEARNING_DB.STG.PURCHASE_MATRIX PM\n            INNER JOIN SNOWFLAKE_LEARNING_DB.STG.TEMP_ACTIVE_DEALERS AD ON PM.DEALER_NO = AD.DEALER_NO\n            INNER JOIN SNOWFLAKE_LEARNING_DB.STG.TEMP_ACTIVE_SKUS ASK ON PM.SKU = ASK.SKU\n        \"\"\").collect()\n        filtered_count = session.sql(\"SELECT COUNT(*) AS CNT FROM SNOWFLAKE_LEARNING_DB.STG.TEMP_FILTERED_PURCHASE_MATRIX\").collect()[0]['CNT']\n        print(f\"Filtered purchase matrix row count: {filtered_count}\")\n\n        # --- User-User Collaborative Filtering ---\n        # Pre-aggregate\n        print(\"Starting pre-aggregation\")\n        session.sql(\"\"\"\n            CREATE OR REPLACE TABLE SNOWFLAKE_LEARNING_DB.STG.TEMP_PRE_PIVOT AS\n            SELECT DEALER_NO, SKU, SUM(PURCHASED) AS PURCHASED\n            FROM SNOWFLAKE_LEARNING_DB.STG.TEMP_FILTERED_PURCHASE_MATRIX\n            GROUP BY DEALER_NO, SKU\n        \"\"\").collect()\n        print(\"Pre-aggregation completed\")\n\n        # Pivot to dealer-SKU matrix\n        print(\"Starting user pivot\")\n        sku_list = session.sql(\"SELECT DISTINCT SKU FROM SNOWFLAKE_LEARNING_DB.STG.TEMP_PRE_PIVOT\").to_pandas()['SKU'].tolist()\n        pivot_columns = \", \".join([f\"SUM(CASE WHEN SKU = '{sku}' THEN PURCHASED ELSE 0 END) AS SKU{sku.replace('SKU', '')}\" for sku in sku_list])\n        session.sql(f\"\"\"\n            CREATE OR REPLACE TABLE SNOWFLAKE_LEARNING_DB.STG.TEMP_PIVOT_MATRIX AS\n            SELECT DEALER_NO, {pivot_columns}\n            FROM SNOWFLAKE_LEARNING_DB.STG.TEMP_PRE_PIVOT\n            GROUP BY DEALER_NO\n        \"\"\").collect()\n        # Fill nulls with 0\n        pivot_cols = session.table(\"SNOWFLAKE_LEARNING_DB.STG.TEMP_PIVOT_MATRIX\")\n        # pivot_cols = session.sql(\"SHOW COLUMNS IN SNOWFLAKE_LEARNING_DB.STG.TEMP_PIVOT_MATRIX\").to_pandas()['column_name'].tolist()\n        update_cols = \", \".join([f\"{c} = COALESCE({c}, 0)\" for c in pivot_cols.columns if c != 'DEALER_NO'])\n        session.sql(f\"\"\"\n            UPDATE SNOWFLAKE_LEARNING_DB.STG.TEMP_PIVOT_MATRIX\n            SET {update_cols}\n        \"\"\").collect()\n        pivot_count = session.sql(\"SELECT COUNT(*) AS CNT FROM SNOWFLAKE_LEARNING_DB.STG.TEMP_PIVOT_MATRIX\").collect()[0]['CNT']\n        print(f\"User pivot matrix schema: {pivot_cols.columns[:10]}... (total {len(pivot_cols.columns)} columns)\")\n        print(f\"User pivot matrix row count: {pivot_count}\")\n\n        # Normalize vectors\n        print(\"Starting user normalization\")\n        norm_cols = \" + \".join([f\"POWER(SKU{sku.replace('SKU', '')}, 2)\" for sku in sku_list])\n        session.sql(f\"\"\"\n            CREATE OR REPLACE TEMPORARY TABLE SNOWFLAKE_LEARNING_DB.STG.TEMP_NORM AS\n            SELECT DEALER_NO, SQRT({norm_cols}) AS NORM\n            FROM SNOWFLAKE_LEARNING_DB.STG.TEMP_PIVOT_MATRIX\n            WHERE SQRT({norm_cols}) > 0\n        \"\"\").collect()\n        norm_count = session.sql(\"SELECT COUNT(*) AS CNT FROM SNOWFLAKE_LEARNING_DB.STG.TEMP_NORM\").collect()[0]['CNT']\n        print(f\"User norm row count: {norm_count}\")\n\n        # Compute dealer similarity\n        print(\"Starting dealer similarity\")\n        dot_product = \" + \".join([f\"A.SKU{sku.replace('SKU', '')} * B.SKU{sku.replace('SKU', '')}\" for sku in sku_list])\n        session.sql(f\"\"\"\n            CREATE OR REPLACE TABLE SNOWFLAKE_LEARNING_DB.STG.TEMP_DEALER_PAIRS AS\n            SELECT \n                A.DEALER_NO AS DEALER_A,\n                B.DEALER_NO AS DEALER_B,\n                ({dot_product} / (NA.NORM * NB.NORM)) AS SIMILARITY\n            FROM SNOWFLAKE_LEARNING_DB.STG.TEMP_PIVOT_MATRIX A\n            CROSS JOIN SNOWFLAKE_LEARNING_DB.STG.TEMP_PIVOT_MATRIX B\n            INNER JOIN SNOWFLAKE_LEARNING_DB.STG.TEMP_NORM NA ON A.DEALER_NO = NA.DEALER_NO\n            INNER JOIN SNOWFLAKE_LEARNING_DB.STG.TEMP_NORM NB ON B.DEALER_NO = NB.DEALER_NO\n            WHERE A.DEALER_NO < B.DEALER_NO\n            AND ({dot_product} / (NA.NORM * NB.NORM)) > 0.2\n        \"\"\").collect()\n        dealer_pairs_count = session.sql(\"SELECT COUNT(*) AS CNT FROM SNOWFLAKE_LEARNING_DB.STG.TEMP_DEALER_PAIRS\").collect()[0]['CNT']\n        print(f\"Dealer pairs row count: {dealer_pairs_count}\")\n\n        # Generate user-user recommendations\n        print(\"Starting user-user recommendations\")\n        session.sql(\"\"\"\n            CREATE OR REPLACE TEMPORARY TABLE SNOWFLAKE_LEARNING_DB.STG.TEMP_USER_RECOMMENDATIONS AS\n            SELECT \n                DP.DEALER_A AS DEALER_NO,\n                PM.SKU,\n                SUM(DP.SIMILARITY) AS RECOMMENDATION_SCORE,\n                'USER_USER' AS RECOMMENDATION_TYPE\n            FROM SNOWFLAKE_LEARNING_DB.STG.TEMP_DEALER_PAIRS DP\n            INNER JOIN SNOWFLAKE_LEARNING_DB.STG.TEMP_FILTERED_PURCHASE_MATRIX PM ON DP.DEALER_B = PM.DEALER_NO\n            WHERE PM.PURCHASED = 1\n            GROUP BY DP.DEALER_A, PM.SKU\n            HAVING SUM(DP.SIMILARITY) > 0\n        \"\"\").collect()\n        session.sql(\"\"\"\n            CREATE OR REPLACE TABLE SNOWFLAKE_LEARNING_DB.STG.TEMP_USER_USER_RECOMMENDATIONS AS\n            SELECT \n                UR.DEALER_NO, UR.SKU, UR.RECOMMENDATION_SCORE, UR.RECOMMENDATION_TYPE\n            FROM SNOWFLAKE_LEARNING_DB.STG.TEMP_USER_RECOMMENDATIONS UR\n            LEFT JOIN SNOWFLAKE_LEARNING_DB.STG.TEMP_FILTERED_PURCHASE_MATRIX PM\n            ON UR.DEALER_NO = PM.DEALER_NO AND UR.SKU = PM.SKU AND PM.PURCHASED = 1\n            WHERE PM.DEALER_NO IS NULL\n        \"\"\").collect()\n        user_rec_count = session.sql(\"SELECT COUNT(*) AS CNT FROM SNOWFLAKE_LEARNING_DB.STG.TEMP_USER_USER_RECOMMENDATIONS\").count()\n        print(f\"User-user recommendations count: {user_rec_count}\")\n\n        # --- Item-Item Collaborative Filtering ---\n        print(\"Starting item pivot\")\n        dealer_list = session.sql(\"SELECT DISTINCT DEALER_NO FROM SNOWFLAKE_LEARNING_DB.STG.TEMP_PRE_PIVOT\").to_pandas()['DEALER_NO'].tolist()\n        item_pivot_columns = \", \".join([f\"SUM(CASE WHEN DEALER_NO = '{dealer}' THEN PURCHASED ELSE 0 END) AS D{dealer.replace('DEALER_NO', '')}\" for dealer in dealer_list])\n        session.sql(f\"\"\"\n            CREATE OR REPLACE TABLE SNOWFLAKE_LEARNING_DB.STG.TEMP_ITEM_PIVOT AS\n            SELECT SKU, {item_pivot_columns}\n            FROM SNOWFLAKE_LEARNING_DB.STG.TEMP_PRE_PIVOT\n            GROUP BY SKU\n        \"\"\").collect()\n        \n        item_pivot_cols = session.table(\"SNOWFLAKE_LEARNING_DB.STG.TEMP_ITEM_PIVOT\")\n        # item_pivot_cols = session.sql(\"SHOW COLUMNS IN SNOWFLAKE_LEARNING_DB.STG.TEMP_ITEM_PIVOT\").to_pandas()['column_name'].tolist()\n        item_update_cols = \", \".join([f\"{c} = COALESCE({c}, 0)\" for c in item_pivot_cols.columns if c != 'SKU'])\n        session.sql(f\"\"\"\n            UPDATE SNOWFLAKE_LEARNING_DB.STG.TEMP_ITEM_PIVOT\n            SET {item_update_cols}\n        \"\"\").collect()\n        item_pivot_count = session.sql(\"SELECT COUNT(*) AS CNT FROM SNOWFLAKE_LEARNING_DB.STG.TEMP_ITEM_PIVOT\").collect()[0]['CNT']\n        print(f\"Item pivot matrix row count: {item_pivot_count}\")\n\n        print(\"Starting item normalization\")\n        item_norm_cols = \" + \".join([f\"POWER(D{dealer.replace('DEALER_', '')}, 2)\" for dealer in dealer_list])\n        session.sql(f\"\"\"\n            CREATE OR REPLACE TEMPORARY TABLE SNOWFLAKE_LEARNING_DB.STG.TEMP_ITEM_NORM AS\n            SELECT SKU, SQRT({item_norm_cols}) AS norm_column\n            FROM SNOWFLAKE_LEARNING_DB.STG.TEMP_ITEM_PIVOT\n            WHERE SQRT({item_norm_cols}) > 0\n        \"\"\").collect()\n        item_norm_count = session.sql(\"SELECT COUNT(*) AS CNT FROM SNOWFLAKE_LEARNING_DB.STG.TEMP_ITEM_NORM\").collect()[0]['CNT']\n        print(f\"Item norm count: {item_norm_count}\")\n\n        print(\"Starting SKU similarity\")\n        item_dot_product = \" + \".join([f\"A.D{dealer.replace('DEALER_', '')} * B.D{dealer.replace('DEALER_', '')}\" for dealer in dealer_list])\n        session.sql(f\"\"\"\n            CREATE OR REPLACE TABLE SNOWFLAKE_LEARNING_DB.STG.TEMP_SKU_PAIRS AS\n            SELECT \n                A.SKU AS SKU_A,\n                B.SKU AS SKU_B,\n                ({item_dot_product} / (NA.norm_column * NB.norm_column)) AS SIMILARITY\n            FROM SNOWFLAKE_LEARNING_DB.STG.TEMP_ITEM_PIVOT A\n            CROSS JOIN SNOWFLAKE_LEARNING_DB.STG.TEMP_ITEM_PIVOT B\n            INNER JOIN SNOWFLAKE_LEARNING_DB.STG.TEMP_ITEM_NORM NA ON A.SKU = NA.SKU\n            INNER JOIN SNOWFLAKE_LEARNING_DB.STG.TEMP_ITEM_NORM NB ON B.SKU = NB.SKU\n            WHERE A.SKU < B.SKU\n            AND ({item_dot_product} / (NA.norm_column * NB.norm_column)) > 0.2\n        \"\"\").collect()\n        sku_pairs_count = session.sql(\"SELECT COUNT(*) AS CNT FROM SNOWFLAKE_LEARNING_DB.STG.TEMP_SKU_PAIRS\").collect()[0]['CNT']\n        print(f\"SKU pairs row count: {sku_pairs_count}\")\n\n        print(\"Starting item-item recommendations\")\n        session.sql(\"\"\"\n            CREATE OR REPLACE TEMPORARY TABLE SNOWFLAKE_LEARNING_DB.STG.TEMP_ITEM_RECOMMENDATIONS AS\n            SELECT \n                PM.DEALER_NO,\n                SP.SKU_B AS SKU,\n                SUM(SP.SIMILARITY) AS RECOMMENDATION_SCORE,\n                'ITEM_ITEM' AS RECOMMENDATION_TYPE\n            FROM SNOWFLAKE_LEARNING_DB.STG.TEMP_SKU_PAIRS SP\n            INNER JOIN SNOWFLAKE_LEARNING_DB.STG.TEMP_FILTERED_PURCHASE_MATRIX PM ON SP.SKU_A = PM.SKU\n            WHERE PM.PURCHASED = 1\n            GROUP BY PM.DEALER_NO, SP.SKU_B\n            HAVING SUM(SP.SIMILARITY) > 0\n        \"\"\").collect()\n        session.sql(\"\"\"\n            CREATE OR REPLACE TABLE SNOWFLAKE_LEARNING_DB.STG.TEMP_ITEM_ITEM_RECOMMENDATIONS AS\n            SELECT \n                IR.DEALER_NO, IR.SKU, IR.RECOMMENDATION_SCORE, IR.RECOMMENDATION_TYPE\n            FROM SNOWFLAKE_LEARNING_DB.STG.TEMP_ITEM_RECOMMENDATIONS IR\n            LEFT JOIN SNOWFLAKE_LEARNING_DB.STG.TEMP_FILTERED_PURCHASE_MATRIX PM\n            ON IR.DEALER_NO = PM.DEALER_NO AND IR.SKU = PM.SKU AND PM.PURCHASED = 1\n            WHERE PM.DEALER_NO IS NULL\n        \"\"\").collect()\n        item_rec_count = session.sql(\"SELECT COUNT(*) AS CNT FROM SNOWFLAKE_LEARNING_DB.STG.TEMP_ITEM_ITEM_RECOMMENDATIONS\").collect()[0]['CNT']\n        print(f\"Item-item recommendations row count: {item_rec_count}\")\n\n        # Combine recommendations\n        session.sql(\"\"\"\n            CREATE OR REPLACE TEMPORARY TABLE SNOWFLAKE_LEARNING_DB.STG.TEMP_COMBINED_RECOMMENDATIONS AS\n            SELECT DEALER_NO, SKU, RECOMMENDATION_SCORE, RECOMMENDATION_TYPE\n            FROM SNOWFLAKE_LEARNING_DB.STG.TEMP_USER_USER_RECOMMENDATIONS\n            UNION\n            SELECT DEALER_NO, SKU, RECOMMENDATION_SCORE, RECOMMENDATION_TYPE\n            FROM SNOWFLAKE_LEARNING_DB.STG.TEMP_ITEM_ITEM_RECOMMENDATIONS\n        \"\"\").collect()\n        combined_count = session.sql(\"SELECT COUNT(*) AS CNT FROM SNOWFLAKE_LEARNING_DB.STG.TEMP_COMBINED_RECOMMENDATIONS\").collect()[0]['CNT']\n        print(f\"Combined recommendations row count: {combined_count}\")\n\n        # Enrich with metadata\n        print(\"Starting metadata enrichment\")\n        session.sql(\"\"\"\n            CREATE OR REPLACE TABLE SNOWFLAKE_LEARNING_DB.DWH.COLLABORATIVE_RECOMMENDATIONS AS\n            SELECT \n                CR.DEALER_NO,\n                CR.SKU,\n                CR.RECOMMENDATION_SCORE,\n                CR.RECOMMENDATION_TYPE,\n                P.PRODUCT_NAME,\n                P.PROD_CATEGORY,\n                P.PROD_SUBCATEGORY,\n                D.DEALER_NAME,\n                D.REGION,\n                D.ADDRESS\n            FROM SNOWFLAKE_LEARNING_DB.STG.TEMP_COMBINED_RECOMMENDATIONS CR\n            LEFT JOIN SNOWFLAKE_LEARNING_DB.ODS.PRODUCTS P ON CR.SKU = P.SKU\n            LEFT JOIN SNOWFLAKE_LEARNING_DB.STG.DEALERS D ON CR.DEALER_NO = D.DEALER_NO\n            ORDER BY CR.DEALER_NO, CR.RECOMMENDATION_SCORE DESC\n        \"\"\").collect()\n        print(\"Saved COLLABORATIVE_RECOMMENDATIONS table to DWH schema\")\n\n        # Validation\n        print(\"Starting validation\")\n        validation_stats = session.sql(\"\"\"\n            SELECT \n                COUNT(DISTINCT DEALER_NO) AS DEALER_COUNT,\n                COUNT(DISTINCT SKU) AS SKU_COUNT,\n                MIN(RECOMMENDATION_SCORE) AS MIN_SCORE,\n                MAX(RECOMMENDATION_SCORE) AS MAX_SCORE,\n                AVG(RECOMMENDATION_SCORE) AS AVG_SCORE\n            FROM SNOWFLAKE_LEARNING_DB.DWH.COLLABORATIVE_RECOMMENDATIONS\n        \"\"\").to_pandas()\n        print(f\"Recommendation validation stats:\\n{validation_stats}\")\n\n        # Sample output\n        sample_recommendations = session.sql(\"\"\"\n            SELECT * FROM SNOWFLAKE_LEARNING_DB.DWH.COLLABORATIVE_RECOMMENDATIONS\n            LIMIT 10\n        \"\"\").to_pandas()\n        print(f\"Sample of COLLABORATIVE_RECOMMENDATIONS:\\n{sample_recommendations}\")\n\n        # Clean up temporary tables\n        # session.sql(\"DROP TABLE IF EXISTS SNOWFLAKE_LEARNING_DB.STG.TEMP_PRE_PIVOT\").collect()\n        # session.sql(\"DROP TABLE IF EXISTS SNOWFLAKE_LEARNING_DB.STG.TEMP_PIVOT_MATRIX\").collect()\n        # session.sql(\"DROP TABLE IF EXISTS SNOWFLAKE_LEARNING_DB.STG.TEMP_ITEM_PIVOT\").collect()\n        # session.sql(\"DROP TABLE IF EXISTS SNOWFLAKE_LEARNING_DB.STG.TEMP_ACTIVE_DEALERS\").collect()\n        # session.sql(\"DROP TABLE IF EXISTS SNOWFLAKE_LEARNING_DB.STG.TEMP_ACTIVE_SKUS\").collect()\n        # session.sql(\"DROP TABLE IF EXISTS SNOWFLAKE_LEARNING_DB.STG.TEMP_FILTERED_PURCHASE_MATRIX\").collect()\n        # session.sql(\"DROP TABLE IF EXISTS SNOWFLAKE_LEARNING_DB.STG.TEMP_NORM\").collect()\n        # session.sql(\"DROP TABLE IF EXISTS SNOWFLAKE_LEARNING_DB.STG.TEMP_DEALER_PAIRS\").collect()\n        # session.sql(\"DROP TABLE IF EXISTS SNOWFLAKE_LEARNING_DB.STG.TEMP_USER_RECOMMENDATIONS\").collect()\n        # session.sql(\"DROP TABLE IF EXISTS SNOWFLAKE_LEARNING_DB.STG.TEMP_USER_USER_RECOMMENDATIONS\").collect()\n        # session.sql(\"DROP TABLE IF EXISTS SNOWFLAKE_LEARNING_DB.STG.TEMP_ITEM_NORM\").collect()\n        # session.sql(\"DROP TABLE IF EXISTS SNOWFLAKE_LEARNING_DB.STG.TEMP_SKU_PAIRS\").collect()\n        # session.sql(\"DROP TABLE IF EXISTS SNOWFLAKE_LEARNING_DB.STG.TEMP_ITEM_RECOMMENDATIONS\").collect()\n        # session.sql(\"DROP TABLE IF EXISTS SNOWFLAKE_LEARNING_DB.STG.TEMP_ITEM_ITEM_RECOMMENDATIONS\").collect()\n        # session.sql(\"DROP TABLE IF EXISTS SNOWFLAKE_LEARNING_DB.STG.TEMP_COMBINED_RECOMMENDATIONS\").collect()\n        # print(\"Cleaned up temporary tables\")\n\n        print(f\"collaborative_filtering completed in {time.time() - start_time:.2f} seconds\")\n    except Exception as e:\n        print(f\"Error in collaborative filtering: {str(e)}\")\n        raise\n    finally:\n        print(\"End of code.....\")\n\ncollaborative_filtering(session)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6d4ff5b0-0d92-4430-9e49-f0587228d19a",
   "metadata": {
    "language": "python",
    "name": "Create_dash_view",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "def create_dashboard_view(session):\n    start_time = time.time()\n    try:\n        session.sql(\"\"\"\n            CREATE OR REPLACE VIEW SNOWFLAKE_LEARNING_DB.DWH.RECOMMENDATION_DASHBOARD AS\n            SELECT \n                r.DEALER_NO,\n                r.DEALER_NAME,\n                r.REGION,\n                r.ADDRESS,\n                r.SKU,\n                r.PRODUCT_NAME,\n                r.PROD_CATEGORY,\n                r.PROD_SUBCATEGORY,\n                r.RECOMMENDATION_SCORE,\n                r.RECOMMENDATION_TYPE,\n                i.TOTAL_SALES,\n                i.TOTAL_INVOICES,\n                i.AVERAGE_BILL_VALUE,\n                i.VERTICAL\n            FROM SNOWFLAKE_LEARNING_DB.DWH.COLLABORATIVE_RECOMMENDATIONS r\n            LEFT JOIN SNOWFLAKE_LEARNING_DB.DWH.RECOMMENDATION_INPUT i\n            ON r.DEALER_NO = i.DEALER_NO AND r.SKU = i.SKU\n            WHERE r.RECOMMENDATION_SCORE > 0.5\n        \"\"\").collect()\n        print(\"Created RECOMMENDATION_DASHBOARD view in DWH schema\")\n\n        sample_view = session.sql(\"SELECT * FROM SNOWFLAKE_LEARNING_DB.DWH.RECOMMENDATION_DASHBOARD LIMIT 10\").to_pandas()\n        print(f\"Sample of RECOMMENDATION_DASHBOARD view:\\n{sample_view}\")\n\n        print(f\"create_dashboard_view completed in {time.time() - start_time:.2f} seconds\")\n    except Exception as e:\n        print(f\"Error creating dashboard view: {str(e)}\")\n        raise\n\ncreate_dashboard_view(session)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "641576e5-ee2c-4638-90f4-48eef0150bc4",
   "metadata": {
    "language": "python",
    "name": "prepare_clustering_filter_for_future_changes"
   },
   "outputs": [],
   "source": "# from snowflake.snowpark.functions import col, sum as sum_, count_distinct, coalesce\n\n# def prepare_clustering_features(session):\n#     start_time = time.time()\n#     try:\n#         # Load dealer features and purchase matrix\n#         dealer_features = session.table(\"SNOWFLAKE_LEARNING_DB.STG.DEALER_FEATURES\")\n#         purchase_matrix = session.table(\"SNOWFLAKE_LEARNING_DB.STG.PURCHASE_MATRIX\")\n\n#         # Create dealer-SKU purchase vector\n#         dealer_sku_vector = purchase_matrix.group_by(\"DEALER_NO\").pivot(\"SKU\").agg(sum_(\"PURCHASED\"))\n#         for c in dealer_sku_vector.columns[1:]:\n#             dealer_sku_vector = dealer_sku_vector.with_column(c, coalesce(col(c), lit(0)))\n\n#         # Combine with dealer features\n#         clustering_features = dealer_features.select(\n#             col(\"DEALER_NO\"),\n#             col(\"TOTAL_SALES\"),\n#             col(\"TOTAL_INVOICES\"),\n#             col(\"AVERAGE_BILL_VALUE\"),\n#             col(\"AVERAGE_SKUS_PER_INVOICE\")\n#         ).join(\n#             dealer_sku_vector,\n#             \"DEALER_NO\",\n#             \"left\"\n#         )\n#         for c in clustering_features.columns[1:]:\n#             clustering_features = clustering_features.with_column(c, coalesce(col(c), lit(0)))\n\n#         # Save to DWH\n#         clustering_features.write.mode(\"overwrite\").save_as_table(\"SNOWFLAKE_LEARNING_DB.DWH.CLUSTERING_FEATURES\")\n#         print(\"Saved CLUSTERING_FEATURES table to DWH schema\")\n#         print(f\"Clustering features row count: {clustering_features.count()}\")\n\n#         # Sample output\n#         sample_features = clustering_features.limit(10).to_pandas()\n#         print(f\"Sample of CLUSTERING_FEATURES:\\n{sample_features}\")\n\n#         print(f\"prepare_clustering_features completed in {time.time() - start_time:.2f} seconds\")\n#     except Exception as e:\n#         print(f\"Error preparing clustering features: {str(e)}\")\n#         raise\n\n# prepare_clustering_features(session)\n",
   "execution_count": null
  }
 ]
}