{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "t4yul2jew7fhsrpcz4dy",
   "authorId": "405902089195",
   "authorName": "LALIT",
   "authorEmail": "lalit.kumar@nihilent.com",
   "sessionId": "eeaa9b5f-8f15-4e80-b2bc-0a1268e2d3ed",
   "lastEditTime": 1749832669248
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "988dd9b1-cea3-4974-a818-b9bbd6b0b743",
   "metadata": {
    "language": "python",
    "name": "Session"
   },
   "outputs": [],
   "source": "from snowflake.snowpark import Session\nfrom snowflake.snowpark.types import StructType, StructField, StringType, IntegerType, FloatType, DateType\nfrom faker import Faker\nfrom snowflake.snowpark.functions import col, count, sum as sum_, count_distinct, when\nfrom datetime import datetime, timedelta\nimport time\nimport pandas as pd\nimport numpy as np\nimport random\n\n# Connect to Snowflake\nconnection_parameters = {\n    \"account\" : \"FVQCWWK-IJB71419\",\n    \"user\" : \"LALIT\",\n    \"authenticator\" : \"Klalitkumar@2025\",\n    \"role\" : \"SNOWFLAKE_LEARNING_ROLE\",\n    \"warehouse\" : \"SNOWFLAKE_LEARNING_WH\",\n    \"database\" : \"SNOWFLAKE_LEARNING_DB\",\n    \"schema\" : \"ODS\"\n}\n\nsession = Session.builder.configs(connection_parameters).create()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "363d58ca-a5a4-4ed5-8b15-87a8bb440dd3",
   "metadata": {
    "language": "python",
    "name": "Parameters"
   },
   "outputs": [],
   "source": "\n# Parameters\nNUM_DEALERS = 5000\nNUM_SKUS = 80\nNUM_NEW_SALES = 5000\nBASELINE_SALES = 10000\nINCREMENTAL_START_DATE = datetime(2025, 5, 10)\nINCREMENTAL_END_DATE = datetime(2025, 6, 10)\nBASELINE_START_DATE = datetime(2024, 1, 1)\nBASELINE_END_DATE = INCREMENTAL_START_DATE - timedelta(days=1)\nBATCH_SIZE = 10000  # Batch size for Snowflake writes\n\n# Initialize Faker and set seeds for reproducibility\nfake = Faker('en_IN')  # Use Indian locale for realistic names and addresses\nFaker.seed(42)\nnp.random.seed(42)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bf122cce-e3cd-4bad-820c-8e44e8f6f2de",
   "metadata": {
    "language": "python",
    "name": "Create_tables",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Check and Create Tables with Uppercase Column Names\ndef create_tables(session):\n    start_time = time.time()\n    try:\n        # Cache table existence check\n        tables_df = session.sql(\"\"\"\n            SELECT TABLE_NAME \n            FROM INFORMATION_SCHEMA.TABLES \n            WHERE TABLE_SCHEMA = 'ODS' \n            AND TABLE_NAME IN ('DEALERS', 'PRODUCTS', 'SALES', 'DP_MAPPING', 'EC_CLUB')\n        \"\"\").to_pandas()\n        tables_exist = tables_df['TABLE_NAME'].tolist()\n        print(f\"Existing tables in ODS schema: {tables_exist}\")\n\n        # Create missing tables\n        if 'DEALERS' not in tables_exist:\n            session.sql(\"\"\"\n                CREATE TABLE IF NOT EXISTS SNOWFLAKE_LEARNING_DB.ODS.DEALERS (\n                    DEALER_NO VARCHAR(50) PRIMARY KEY,\n                    DEALER_NAME VARCHAR(100),\n                    REGION VARCHAR(50),\n                    ADDRESS VARCHAR(200),\n                    ANNUAL_REVENUE DECIMAL(18,2),\n                    DEALER_TYPE VARCHAR(20),\n                    EC_CLUB INTEGER,\n                    TOTAL_SALES DECIMAL(18,2) DEFAULT 0,\n                    TOTAL_INVOICES INTEGER DEFAULT 0,\n                    AVERAGE_BILL_VALUE DECIMAL(18,2) DEFAULT 0,\n                    AVERAGE_SKUS_PER_INVOICE DECIMAL(18,2) DEFAULT 0\n                )\n            \"\"\").collect()\n            print(\"Created table ODS.DEALERS\")\n\n        if 'PRODUCTS' not in tables_exist:\n            session.sql(\"\"\"\n                CREATE TABLE IF NOT EXISTS SNOWFLAKE_LEARNING_DB.ODS.PRODUCTS (\n                    SKU VARCHAR(50) PRIMARY KEY,\n                    PRODUCT_NAME VARCHAR(100),\n                    PROD_CATEGORY VARCHAR(50),\n                    PROD_RANGE VARCHAR(50),\n                    PROD_SUBCATEGORY VARCHAR(50),\n                    IS_TRADING INTEGER,\n                    IS_FINISHED INTEGER\n                )\n            \"\"\").collect()\n            print(\"Created table ODS.PRODUCTS\")\n\n        if 'SALES' not in tables_exist:\n            session.sql(\"\"\"\n                CREATE TABLE IF NOT EXISTS SNOWFLAKE_LEARNING_DB.ODS.SALES (\n                    DEALER_NO VARCHAR(50),\n                    SKU VARCHAR(50),\n                    DATE DATE,\n                    QUANTITY INTEGER,\n                    AMOUNT DECIMAL(18,2),\n                    PRIMARY KEY (DEALER_NO, SKU, DATE)\n                )\n            \"\"\").collect()\n            print(\"Created table ODS.SALES\")\n\n        if 'DP_MAPPING' not in tables_exist:\n            session.sql(\"\"\"\n                CREATE TABLE IF NOT EXISTS SNOWFLAKE_LEARNING_DB.ODS.DP_MAPPING (\n                    DN_NUMBER VARCHAR(50) PRIMARY KEY,\n                    VERTICAL VARCHAR(50)\n                )\n            \"\"\").collect()\n            print(\"Created table ODS.DP_MAPPING\")\n\n        if 'EC_CLUB' not in tables_exist:\n            session.sql(\"\"\"\n                CREATE TABLE IF NOT EXISTS SNOWFLAKE_LEARNING_DB.ODS.EC_CLUB (\n                    DEALER_NO VARCHAR(50) PRIMARY KEY,\n                    EC_CLUB INTEGER\n                )\n            \"\"\").collect()\n            print(\"Created table ODS.EC_CLUB\")\n        print(f\"create_tables completed in {time.time() - start_time:.2f} seconds\")\n    except Exception as e:\n        print(f\"Error creating tables: {str(e)}\")\n        raise",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6422fd97-74ce-4f2b-9b75-b6f4c8ccfb4e",
   "metadata": {
    "language": "python",
    "name": "get_baseline_data"
   },
   "outputs": [],
   "source": "# Generating baseline data if tables are empty.\ndef generate_baseline_data(session):\n    start_time = time.time()\n    try:\n        # Step 1: Check existing table data counts\n        table_counts = {\n            'DEALERS': session.sql(\"SELECT COUNT(*) AS CNT FROM SNOWFLAKE_LEARNING_DB.ODS.DEALERS\").collect()[0][\"CNT\"],\n            'PRODUCTS': session.sql(\"SELECT COUNT(*) AS CNT FROM SNOWFLAKE_LEARNING_DB.ODS.PRODUCTS\").collect()[0][\"CNT\"],\n            'SALES': session.sql(\"SELECT COUNT(*) AS CNT FROM SNOWFLAKE_LEARNING_DB.ODS.SALES\").collect()[0][\"CNT\"]\n        }\n        print(f\"Table counts: {table_counts}\")\n\n        if any(count == 0 for count in table_counts.values()):\n            # Region & category setup\n            regions = ['MAHARASHTRA', 'DELHI', 'KARNATAKA', 'TAMIL_NADU', 'GUJARAT', 'UTTAR_PRADESH', 'WEST_BENGAL', 'KERALA', 'RAJASTHAN', 'PUNJAB']\n            dealer_types = ['RETAILER', 'WHOLESALER', 'DISTRIBUTOR']\n            verticals = ['PLUMBING', 'WATER_TANK', 'AGRI', 'BATHROOM_PRODUCTS']\n            product_categories = ['Sanitary', 'Plumbing', 'Bathroom', 'Agriculture', 'Tiling', 'Water Storage']\n            product_subcategories = {\n                'Sanitary': ['Taps', 'Showers', 'Faucets'],\n                'Plumbing': ['Pipes', 'Fittings', 'Valves'],\n                'Bathroom': ['Basins', 'Commodes', 'Urinals'],\n                'Agriculture': ['Drip Irrigation', 'Sprinklers', 'PVC Pipes'],\n                'Tiling': ['Wall Tiles', 'Floor Tiles', 'Ceramic'],\n                'Water Storage': ['Tanks', 'Overhead Tanks', 'Underground Tanks']\n            }\n            product_ranges = ['Economy', 'Standard', 'Premium']\n\n            # Generate dealers\n            dealer_df = pd.DataFrame({\n                'DEALER_NO': [f'D{str(i+1).zfill(6)}' for i in range(NUM_DEALERS)],\n                'DEALER_NAME': [fake.company() for _ in range(NUM_DEALERS)],\n                'REGION': np.random.choice(regions, NUM_DEALERS),\n                'ADDRESS': [fake.address().replace('\\n', ', ') for _ in range(NUM_DEALERS)],\n                'ANNUAL_REVENUE': np.round(np.random.lognormal(12, 1, NUM_DEALERS), 2),\n                'DEALER_TYPE': np.random.choice(dealer_types, NUM_DEALERS),\n                'EC_CLUB': np.random.choice([1, 0], NUM_DEALERS, p=[0.2, 0.8]),\n                'TOTAL_SALES': 0,\n                'TOTAL_INVOICES': 0,\n                'AVERAGE_BILL_VALUE': 0,\n                'AVERAGE_SKUS_PER_INVOICE': 0\n            })\n\n            # Generate products\n            product_data = []\n            for _ in range(NUM_SKUS):\n                cat = random.choice(product_categories)\n                subcat = random.choice(product_subcategories[cat])\n                range_ = random.choice(product_ranges)\n                product_data.append((cat, subcat, range_, f\"{subcat} ({range_})\"))\n            products_df = pd.DataFrame(product_data, columns=['PROD_CATEGORY', 'PROD_SUBCATEGORY', 'PROD_RANGE', 'PRODUCT_NAME'])\n            products_df.insert(0, 'SKU', [f'SKU{str(i+1).zfill(5)}' for i in range(NUM_SKUS)])\n            flags = np.random.choice([0, 1], size=len(products_df))\n            products_df['IS_TRADING'] = flags\n            products_df['IS_FINISHED'] = 1 - flags\n\n\n            # Generate sales\n            dealer_list = dealer_df['DEALER_NO'].values\n            sku_list = products_df['SKU'].values\n            sales_data = []\n            date_range = (BASELINE_END_DATE - BASELINE_START_DATE).days\n            for i in range(0, BASELINE_SALES, BATCH_SIZE):\n                batch_size = min(BATCH_SIZE, BASELINE_SALES - i)\n                sales_batch = pd.DataFrame({\n                    'DEALER_NO': np.random.choice(dealer_list, batch_size),\n                    'SKU': np.random.choice(sku_list, batch_size),\n                    'DATE': [BASELINE_START_DATE + timedelta(days=random.randint(0, date_range)) for _ in range(batch_size)],\n                    'QUANTITY': np.random.randint(1, 51, batch_size),\n                    'AMOUNT': np.round(np.random.uniform(10, 500, batch_size), 2)\n                })\n                sales_batch['AMOUNT'] *= sales_batch['QUANTITY']\n                sales_data.append(sales_batch)\n            sales_df = pd.concat(sales_data, ignore_index=True)\n\n            # Additional mappings\n            dp_mapping_df = pd.DataFrame({\n                'DN_NUMBER': dealer_list,\n                'VERTICAL': np.random.choice(verticals, NUM_DEALERS)\n            })\n            ec_club_df = dealer_df[['DEALER_NO', 'EC_CLUB']]\n\n            # Write data to Snowflake\n            for df, table in [\n                (dealer_df, \"SNOWFLAKE_LEARNING_DB.ODS.DEALERS\"),\n                (products_df, \"SNOWFLAKE_LEARNING_DB.ODS.PRODUCTS\"),\n                (dp_mapping_df, \"SNOWFLAKE_LEARNING_DB.ODS.DP_MAPPING\"),\n                (ec_club_df, \"SNOWFLAKE_LEARNING_DB.ODS.EC_CLUB\")\n            ]:\n                session.create_dataframe(df).write.mode(\"overwrite\").save_as_table(table)\n                print(f\"Written {len(df)} rows to {table}\")\n\n            for i in range(0, len(sales_df), BATCH_SIZE):\n                mode = \"append\" if i > 0 else \"overwrite\"\n                session.create_dataframe(sales_df[i:i+BATCH_SIZE]).write.mode(mode).save_as_table(\"SNOWFLAKE_LEARNING_DB.ODS.SALES\")\n                print(f\"Wrote sales batch {i//BATCH_SIZE + 1}\")\n\n            print(f\"Baseline data generated in {time.time() - start_time:.2f} seconds\")\n            return dealer_df, products_df, sales_df\n\n        else:\n            # Load from Snowflake if data exists\n            dealer_df = session.table(\"SNOWFLAKE_LEARNING_DB.ODS.DEALERS\").to_pandas()\n            products_df = session.table(\"SNOWFLAKE_LEARNING_DB.ODS.PRODUCTS\").to_pandas()\n            sales_df = session.table(\"SNOWFLAKE_LEARNING_DB.ODS.SALES\").to_pandas()\n            print(f\"Loaded dealers: {len(dealer_df)} rows\")\n            print(f\"Loaded products: {len(products_df)} rows\")\n            print(f\"Loaded sales: {len(sales_df)} rows\")\n            return dealer_df, products_df, sales_df\n\n    except Exception as e:\n        print(f\"Error in generate_baseline_data: {str(e)}\")\n        raise\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bdae7071-ed65-4135-8f5b-aa119ce4c498",
   "metadata": {
    "language": "python",
    "name": "get_inc_sales_data",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Generate Incremental Sales data\ndef generate_incremental_sales(session, dealers_df, products_df, sales_df):\n    start_time = time.time()\n    try:\n        if 'DEALER_NO' not in dealers_df.columns:\n            raise KeyError(f\"'DEALER_NO' missing in dealers_df. Columns: {list(dealers_df.columns)}\")\n        dealer_list = dealers_df['DEALER_NO'].values\n        if len(dealer_list) == 0:\n            raise ValueError(\"No dealers available in dealers_df\")\n        sku_list = products_df['SKU'].values\n        if len(sku_list) == 0:\n            raise ValueError(\"No SKUs available in products_df\")\n\n        # Dealer weights\n        dealer_weights = np.ones(len(dealer_list)) if sales_df.empty else sales_df['DEALER_NO'].value_counts().reindex(dealer_list, fill_value=1).values\n        dealer_weights = dealer_weights / dealer_weights.sum()\n\n        # Incremental sales (batch generation)\n        date_range = (INCREMENTAL_END_DATE - INCREMENTAL_START_DATE).days\n        sales_data = []\n        for i in range(0, NUM_NEW_SALES, BATCH_SIZE):\n            batch_size = min(BATCH_SIZE, NUM_NEW_SALES - i)\n            dealer_nos = np.random.choice(dealer_list, batch_size, p=dealer_weights)\n            skus = np.random.choice(sku_list, batch_size)\n            quantities = np.random.randint(1, 51, batch_size)\n            unit_prices = np.round(np.random.uniform(10, 500, batch_size), 2)\n            amounts = np.round(quantities * unit_prices, 2)\n            dates = [INCREMENTAL_START_DATE + timedelta(days=random.randint(0, date_range)) for _ in range(batch_size)]\n            batch_df = pd.DataFrame({\n                'DEALER_NO': dealer_nos,\n                'SKU': skus,\n                'DATE': dates,\n                'QUANTITY': quantities,\n                'AMOUNT': amounts\n            })\n            sales_data.append(batch_df)\n            # Write batch to Snowflake\n            session.create_dataframe(batch_df).write.mode(\"append\").save_as_table(\"SNOWFLAKE_LEARNING_DB.ODS.SALES\")\n            print(f\"Appended {len(batch_df)} incremental sales rows (batch {i//BATCH_SIZE + 1})\")\n\n        new_sales_df = pd.concat(sales_data, ignore_index=True)\n        sales_df = pd.concat([sales_df, new_sales_df], ignore_index=True)\n        print(f\"generate_incremental_sales completed in {time.time() - start_time:.2f} seconds\")\n        return sales_df\n    except Exception as e:\n        print(f\"Error generating incremental sales: {str(e)}\")\n        raise",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "15e80b59-d3ab-47d6-b57a-09435b96e583",
   "metadata": {
    "language": "python",
    "name": "update_dealers_features"
   },
   "outputs": [],
   "source": "# Update Dealer Features\ndef update_dealer_features(session):\n    start_time = time.time()\n    try:\n        # Define the MERGE query with a CTE for dealer_agg\n        merge_query = \"\"\"\n        MERGE INTO SNOWFLAKE_LEARNING_DB.ODS.DEALERS D\n        USING (\n            WITH dealer_agg AS (\n                SELECT \n                    DEALER_NO,\n                    SUM(AMOUNT) AS TOTAL_SALES,\n                    COUNT(*) AS TOTAL_INVOICES,\n                    COUNT(DISTINCT SKU) AS UNIQUE_SKUS,\n                    CASE WHEN COUNT(*) > 0 THEN SUM(AMOUNT) / COUNT(*) ELSE 0 END AS AVERAGE_BILL_VALUE,\n                    CASE WHEN COUNT(*) > 0 THEN COUNT(DISTINCT SKU) / COUNT(*) ELSE 0 END AS AVERAGE_SKUS_PER_INVOICE\n                FROM SNOWFLAKE_LEARNING_DB.ODS.SALES\n                GROUP BY DEALER_NO\n            )\n            SELECT \n                DEALER_NO,\n                TOTAL_SALES,\n                TOTAL_INVOICES,\n                AVERAGE_BILL_VALUE,\n                AVERAGE_SKUS_PER_INVOICE\n            FROM dealer_agg\n        ) U\n        ON D.DEALER_NO = U.DEALER_NO\n        WHEN MATCHED THEN\n            UPDATE SET\n                TOTAL_SALES = U.TOTAL_SALES,\n                TOTAL_INVOICES = U.TOTAL_INVOICES,\n                AVERAGE_BILL_VALUE = U.AVERAGE_BILL_VALUE,\n                AVERAGE_SKUS_PER_INVOICE = U.AVERAGE_SKUS_PER_INVOICE\n        \"\"\"\n        print(\"Executing MERGE query for dealer features update\")\n        session.sql(merge_query).collect()\n        print(f\"update_dealer_features completed in {time.time() - start_time:.2f} seconds\")\n    except Exception as e:\n        print(f\"Error updating dealer features: {str(e)}\")\n        raise",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "178b8469-57cf-4aad-9232-48f6183a7060",
   "metadata": {
    "language": "python",
    "name": "data_validation",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Step 5: Validate Data\ndef validate_data(session):\n    start_time = time.time()\n    try:\n        # Check row counts\n        counts = {\n            'DEALERS': session.sql(\"SELECT COUNT(*) FROM SNOWFLAKE_LEARNING_DB.ODS.DEALERS\").count(),\n            'PRODUCTS': session.sql(\"SELECT COUNT(*) FROM SNOWFLAKE_LEARNING_DB.ODS.PRODUCTS\").count(),\n            'SALES': session.sql(\"SELECT COUNT(*) FROM SNOWFLAKE_LEARNING_DB.ODS.SALES\").count(),\n            'DP_MAPPING': session.sql(\"SELECT COUNT(*) FROM SNOWFLAKE_LEARNING_DB.ODS.DP_MAPPING\").count(),\n            'EC_CLUB': session.sql(\"SELECT COUNT(*) FROM SNOWFLAKE_LEARNING_DB.ODS.EC_CLUB\").count()\n        }\n        print(f\"Table row counts: {counts}\")\n\n        # Check for duplicates in sales\n        duplicates = session.sql(\"\"\"\n            SELECT DEALER_NO, SKU, DATE, COUNT(*)\n            FROM SNOWFLAKE_LEARNING_DB.ODS.SALES\n            GROUP BY DEALER_NO, SKU, DATE\n            HAVING COUNT(*) > 1\n        \"\"\").collect()\n        if duplicates:\n            print(f\"Duplicates found in ODS.SALES: {duplicates}\")\n        else:\n            print(\"No duplicates found in ODS.SALES.\")\n\n        # Verify column names\n        show_cols_df = session.sql(\"SHOW COLUMNS IN SNOWFLAKE_LEARNING_DB.ODS.DEALERS\").to_pandas()\n        print(f\"SHOW COLUMNS DataFrame columns: {list(show_cols_df.columns)}\")\n        if len(show_cols_df.columns) < 3:\n            raise KeyError(f\"Expected at least 3 columns in SHOW COLUMNS for SNOWFLAKE_LEARNING_DB.ODS.DEALERS. Got: {list(show_cols_df.columns)}\")\n        dealer_cols = show_cols_df.iloc[:, 2].tolist()\n        show_cols_df = session.sql(\"SHOW COLUMNS IN SNOWFLAKE_LEARNING_DB.ODS.PRODUCTS\").to_pandas()\n        print(f\"SHOW COLUMNS DataFrame columns: {list(show_cols_df.columns)}\")\n        if len(show_cols_df.columns) < 3:\n            raise KeyError(f\"Expected at least 3 columns in SHOW COLUMNS for SNOWFLAKE_LEARNING_DB.ODS.PRODUCTS. Got: {list(show_cols_df.columns)}\")\n        product_cols = show_cols_df.iloc[:, 2].tolist()\n        print(f\"DEALERS table columns: {dealer_cols}\")\n        print(f\"PRODUCTS table columns: {product_cols}\")\n\n        # Sample data\n        sample_dealers = session.table(\"SNOWFLAKE_LEARNING_DB.ODS.DEALERS\").limit(5).to_pandas()\n        sample_products = session.table(\"SNOWFLAKE_LEARNING_DB.ODS.PRODUCTS\").limit(5).to_pandas()\n        sample_sales = session.table(\"SNOWFLAKE_LEARNING_DB.ODS.SALES\").limit(5).to_pandas()\n        print(\"Sample of ODS.DEALERS:\\n\" + str(sample_dealers))\n        print(\"Sample of ODS.PRODUCTS:\\n\" + str(sample_products))\n        print(\"Sample of ODS.SALES:\\n\" + str(sample_sales))\n        print(f\"validate_data completed in {time.time() - start_time:.2f} seconds\")\n    except Exception as e:\n        print(f\"Error validating data: {str(e)}\")\n        raise",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6bc246b0-12d4-4232-8a45-926092543fc2",
   "metadata": {
    "language": "python",
    "name": "main",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Main Execution\ntry:\n    start_time = time.time()\n    print(\"Starting synthetic data generation...\")\n    \n    # Check table existence\n    tables_df = session.sql(\"\"\"\n        SELECT TABLE_NAME \n        FROM INFORMATION_SCHEMA.TABLES \n        WHERE TABLE_SCHEMA = 'ODS' \n        AND TABLE_NAME IN ('DEALERS', 'PRODUCTS', 'SALES', 'DP_MAPPING', 'EC_CLUB')\n    \"\"\").to_pandas()\n    tables_exist = tables_df['TABLE_NAME'].tolist()\n    print(f\"Tables found in ODS schema: {tables_exist}\")\n    \n    # Create tables if missing\n    if len(tables_exist) < 5:\n        create_tables(session)\n    else:\n        print(\"All required tables exist, skipping table creation.\")\n\n    # Generate or load baseline data\n    dealers_df, products_df, sales_df = generate_baseline_data(session)\n    \n    # Generate incremental sales\n    sales_df = generate_incremental_sales(session, dealers_df, products_df, sales_df)\n    \n    # Update dealer features\n    update_dealer_features(session)\n    \n    # Validate data\n    validate_data(session)\n    \n    print(f\"Synthetic data generation completed in {time.time() - start_time:.2f} seconds\")\nexcept Exception as e:\n    print(f\"Data generation failed: {str(e)}\")\n    raise",
   "execution_count": null
  }
 ]
}